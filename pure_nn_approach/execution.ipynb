{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'five_channels' from 'mns_dataloaders' (/home/yukam/deeplearning/MISTI_Brazil_dPASP_research/MISTI_Brazil_dPASP_research/mns_dataloaders.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[55], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mmns_dataloaders\u001b[39;00m \u001b[39mimport\u001b[39;00m two_concat,three_channels,five_channels,five_concat\n\u001b[1;32m      2\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mmns_models\u001b[39;00m \u001b[39mimport\u001b[39;00m five_concat_Net,two_concat_Net,three_channels_Net,five_channels_Net\n\u001b[1;32m      3\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'five_channels' from 'mns_dataloaders' (/home/yukam/deeplearning/MISTI_Brazil_dPASP_research/MISTI_Brazil_dPASP_research/mns_dataloaders.py)"
     ]
    }
   ],
   "source": [
    "from mns_dataloaders import two_concat,three_channels,five_channels,five_concat\n",
    "from mns_models import five_concat_Net,two_concat_Net,three_channels_Net,five_channels_Net\n",
    "import torch\n",
    "\n",
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'five_concat' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[47], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m train_dataloader,test_dataloader \u001b[39m=\u001b[39m five_concat()\n\u001b[1;32m      2\u001b[0m model \u001b[39m=\u001b[39m five_concat_Net()\u001b[39m.\u001b[39mto(device)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'five_concat' is not defined"
     ]
    }
   ],
   "source": [
    "train_dataloader,test_dataloader = five_concat()\n",
    "model = five_concat_Net().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-3)\n",
    "\n",
    "def train(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    model.train()\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "\n",
    "        # Compute prediction error\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), (batch + 1) * len(X)\n",
    "            # print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "\n",
    "def test(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    model.eval()\n",
    "    test_loss, correct = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 11.8%, Avg loss: 2.280028 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 12.1%, Avg loss: 2.264619 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 13.2%, Avg loss: 2.240451 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[46], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(epochs):\n\u001b[1;32m      3\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mEpoch \u001b[39m\u001b[39m{\u001b[39;00mt\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m-------------------------------\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m----> 4\u001b[0m     train(train_dataloader, model, loss_fn, optimizer)\n\u001b[1;32m      5\u001b[0m     test(test_dataloader, model, loss_fn)\n\u001b[1;32m      6\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mDone!\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[45], line 7\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(dataloader, model, loss_fn, optimizer)\u001b[0m\n\u001b[1;32m      5\u001b[0m size \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(dataloader\u001b[39m.\u001b[39mdataset)\n\u001b[1;32m      6\u001b[0m model\u001b[39m.\u001b[39mtrain()\n\u001b[0;32m----> 7\u001b[0m \u001b[39mfor\u001b[39;00m batch, (X, y) \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(dataloader):\n\u001b[1;32m      8\u001b[0m     X, y \u001b[39m=\u001b[39m X\u001b[39m.\u001b[39mto(device), y\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m     10\u001b[0m     \u001b[39m# Compute prediction error\u001b[39;00m\n",
      "File \u001b[0;32m/usr/lib/python3.11/site-packages/torch/utils/data/dataloader.py:633\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    630\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    631\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    632\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 633\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[1;32m    634\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    635\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    636\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    637\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m/usr/lib/python3.11/site-packages/torch/utils/data/dataloader.py:677\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    675\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    676\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 677\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    678\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[1;32m    679\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m/usr/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py:54\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n\u001b[0;32m---> 54\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcollate_fn(data)\n",
      "File \u001b[0;32m/usr/lib/python3.11/site-packages/torch/utils/data/_utils/collate.py:265\u001b[0m, in \u001b[0;36mdefault_collate\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m    204\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdefault_collate\u001b[39m(batch):\n\u001b[1;32m    205\u001b[0m \u001b[39m    \u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    206\u001b[0m \u001b[39m        Function that takes in a batch of data and puts the elements within the batch\u001b[39;00m\n\u001b[1;32m    207\u001b[0m \u001b[39m        into a tensor with an additional outer dimension - batch size. The exact output type can be\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    263\u001b[0m \u001b[39m            >>> default_collate(batch)  # Handle `CustomType` automatically\u001b[39;00m\n\u001b[1;32m    264\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 265\u001b[0m     \u001b[39mreturn\u001b[39;00m collate(batch, collate_fn_map\u001b[39m=\u001b[39;49mdefault_collate_fn_map)\n",
      "File \u001b[0;32m/usr/lib/python3.11/site-packages/torch/utils/data/_utils/collate.py:142\u001b[0m, in \u001b[0;36mcollate\u001b[0;34m(batch, collate_fn_map)\u001b[0m\n\u001b[1;32m    139\u001b[0m transposed \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(\u001b[39mzip\u001b[39m(\u001b[39m*\u001b[39mbatch))  \u001b[39m# It may be accessed twice, so we use a list.\u001b[39;00m\n\u001b[1;32m    141\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(elem, \u001b[39mtuple\u001b[39m):\n\u001b[0;32m--> 142\u001b[0m     \u001b[39mreturn\u001b[39;00m [collate(samples, collate_fn_map\u001b[39m=\u001b[39;49mcollate_fn_map) \u001b[39mfor\u001b[39;49;00m samples \u001b[39min\u001b[39;49;00m transposed]  \u001b[39m# Backwards compatibility.\u001b[39;00m\n\u001b[1;32m    143\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    144\u001b[0m     \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[0;32m/usr/lib/python3.11/site-packages/torch/utils/data/_utils/collate.py:142\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    139\u001b[0m transposed \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(\u001b[39mzip\u001b[39m(\u001b[39m*\u001b[39mbatch))  \u001b[39m# It may be accessed twice, so we use a list.\u001b[39;00m\n\u001b[1;32m    141\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(elem, \u001b[39mtuple\u001b[39m):\n\u001b[0;32m--> 142\u001b[0m     \u001b[39mreturn\u001b[39;00m [collate(samples, collate_fn_map\u001b[39m=\u001b[39;49mcollate_fn_map) \u001b[39mfor\u001b[39;00m samples \u001b[39min\u001b[39;00m transposed]  \u001b[39m# Backwards compatibility.\u001b[39;00m\n\u001b[1;32m    143\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    144\u001b[0m     \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[0;32m/usr/lib/python3.11/site-packages/torch/utils/data/_utils/collate.py:119\u001b[0m, in \u001b[0;36mcollate\u001b[0;34m(batch, collate_fn_map)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[39mif\u001b[39;00m collate_fn_map \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    118\u001b[0m     \u001b[39mif\u001b[39;00m elem_type \u001b[39min\u001b[39;00m collate_fn_map:\n\u001b[0;32m--> 119\u001b[0m         \u001b[39mreturn\u001b[39;00m collate_fn_map[elem_type](batch, collate_fn_map\u001b[39m=\u001b[39;49mcollate_fn_map)\n\u001b[1;32m    121\u001b[0m     \u001b[39mfor\u001b[39;00m collate_type \u001b[39min\u001b[39;00m collate_fn_map:\n\u001b[1;32m    122\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(elem, collate_type):\n",
      "File \u001b[0;32m/usr/lib/python3.11/site-packages/torch/utils/data/_utils/collate.py:162\u001b[0m, in \u001b[0;36mcollate_tensor_fn\u001b[0;34m(batch, collate_fn_map)\u001b[0m\n\u001b[1;32m    160\u001b[0m     storage \u001b[39m=\u001b[39m elem\u001b[39m.\u001b[39m_typed_storage()\u001b[39m.\u001b[39m_new_shared(numel, device\u001b[39m=\u001b[39melem\u001b[39m.\u001b[39mdevice)\n\u001b[1;32m    161\u001b[0m     out \u001b[39m=\u001b[39m elem\u001b[39m.\u001b[39mnew(storage)\u001b[39m.\u001b[39mresize_(\u001b[39mlen\u001b[39m(batch), \u001b[39m*\u001b[39m\u001b[39mlist\u001b[39m(elem\u001b[39m.\u001b[39msize()))\n\u001b[0;32m--> 162\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49mstack(batch, \u001b[39m0\u001b[39;49m, out\u001b[39m=\u001b[39;49mout)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "epochs = 2\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train(train_dataloader, model, loss_fn, optimizer)\n",
    "    test(test_dataloader, model, loss_fn)\n",
    "print(\"Done!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "\n",
    "# Download training data from open datasets.\n",
    "training_data_atoms = datasets.MNIST(\n",
    "    root=\"data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=ToTensor(),\n",
    ")\n",
    "\n",
    "# Download test data from open datasets.\n",
    "test_data_atoms = datasets.MNIST(\n",
    "    root=\"data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=ToTensor(),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0118, 0.0706, 0.0706, 0.0706,\n",
       "           0.4941, 0.5333, 0.6863, 0.1020, 0.6510, 1.0000, 0.9686, 0.4980,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.1176, 0.1412, 0.3686, 0.6039, 0.6667, 0.9922, 0.9922, 0.9922,\n",
       "           0.9922, 0.9922, 0.8824, 0.6745, 0.9922, 0.9490, 0.7647, 0.2510,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1922,\n",
       "           0.9333, 0.9922, 0.9922, 0.9922, 0.9922, 0.9922, 0.9922, 0.9922,\n",
       "           0.9922, 0.9843, 0.3647, 0.3216, 0.3216, 0.2196, 0.1529, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0706,\n",
       "           0.8588, 0.9922, 0.9922, 0.9922, 0.9922, 0.9922, 0.7765, 0.7137,\n",
       "           0.9686, 0.9451, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.3137, 0.6118, 0.4196, 0.9922, 0.9922, 0.8039, 0.0431, 0.0000,\n",
       "           0.1686, 0.6039, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0549, 0.0039, 0.6039, 0.9922, 0.3529, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.5451, 0.9922, 0.7451, 0.0078, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0431, 0.7451, 0.9922, 0.2745, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.1373, 0.9451, 0.8824, 0.6275,\n",
       "           0.4235, 0.0039, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.3176, 0.9412, 0.9922,\n",
       "           0.9922, 0.4667, 0.0980, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1765, 0.7294,\n",
       "           0.9922, 0.9922, 0.5882, 0.1059, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0627,\n",
       "           0.3647, 0.9882, 0.9922, 0.7333, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.9765, 0.9922, 0.9765, 0.2510, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1804, 0.5098,\n",
       "           0.7176, 0.9922, 0.9922, 0.8118, 0.0078, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.1529, 0.5804, 0.8980, 0.9922,\n",
       "           0.9922, 0.9922, 0.9804, 0.7137, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0941, 0.4471, 0.8667, 0.9922, 0.9922, 0.9922,\n",
       "           0.9922, 0.7882, 0.3059, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0902, 0.2588, 0.8353, 0.9922, 0.9922, 0.9922, 0.9922, 0.7765,\n",
       "           0.3176, 0.0078, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0706, 0.6706,\n",
       "           0.8588, 0.9922, 0.9922, 0.9922, 0.9922, 0.7647, 0.3137, 0.0353,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.2157, 0.6745, 0.8863, 0.9922,\n",
       "           0.9922, 0.9922, 0.9922, 0.9569, 0.5216, 0.0431, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.5333, 0.9922, 0.9922, 0.9922,\n",
       "           0.8314, 0.5294, 0.5176, 0.0627, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000]]]),\n",
       " 5)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data_atoms[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([[[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.3294, 0.7255,\n",
      "          0.6235, 0.5922, 0.2353, 0.1412, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.8706, 0.9961,\n",
      "          0.9961, 0.9961, 0.9961, 0.9451, 0.7765, 0.7765, 0.7765, 0.7765,\n",
      "          0.7765, 0.7765, 0.7765, 0.7765, 0.6667, 0.2039, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2627, 0.4471,\n",
      "          0.2824, 0.4471, 0.6392, 0.8902, 0.9961, 0.8824, 0.9961, 0.9961,\n",
      "          0.9961, 0.9804, 0.8980, 0.9961, 0.9961, 0.5490, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0667, 0.2588, 0.0549, 0.2627, 0.2627,\n",
      "          0.2627, 0.2314, 0.0824, 0.9255, 0.9961, 0.4157, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.3255, 0.9922, 0.8196, 0.0706, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0863, 0.9137, 1.0000, 0.3255, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.5059, 0.9961, 0.9333, 0.1725, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.2314, 0.9765, 0.9961, 0.2431, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.5216, 0.9961, 0.7333, 0.0196, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0353,\n",
      "          0.8039, 0.9725, 0.2275, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.4941,\n",
      "          0.9961, 0.7137, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2941, 0.9843,\n",
      "          0.9412, 0.2235, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0745, 0.8667, 0.9961,\n",
      "          0.6510, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0118, 0.7961, 0.9961, 0.8588,\n",
      "          0.1373, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.1490, 0.9961, 0.9961, 0.3020,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.1216, 0.8784, 0.9961, 0.4510, 0.0039,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.5216, 0.9961, 0.9961, 0.2039, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.2392, 0.9490, 0.9961, 0.9961, 0.2039, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.4745, 0.9961, 0.9961, 0.8588, 0.1569, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.4745, 0.9961, 0.8118, 0.0706, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000]]]), 7)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "img, label = test_data_atoms[0]\n",
    "print(test_data_atoms[0])\n",
    "plt.imshow(img[0], cmap=\"gray\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### separate images by digit labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_digits= [[] for i in range(10)]\n",
    "for img, label in test_data_atoms:\n",
    "    test_digits[label]+=img\n",
    "train_digits= [[] for i in range(10)]\n",
    "for img, label in training_data_atoms:\n",
    "    train_digits[label]+=img\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5923\n",
      "6742\n",
      "5958\n",
      "6131\n",
      "5842\n",
      "5421\n",
      "5918\n",
      "6265\n",
      "5851\n",
      "5949\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    print(len(train_digits[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(test_digits[3][0], cmap=\"gray\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate the set of five numbers, and a dataset of sets consisting five images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def generate_num_set(n):\n",
    "    small = max(n-NUM_RANGE[1],0)\n",
    "    big = min(NUM_RANGE[1],n - small)\n",
    "    num1 = random.randint(small,big)\n",
    "    num2 = random.randint(small,big)\n",
    "    num3 = random.randint(small,big)\n",
    "    return (num1,n-num1,num2,n-num2,num3,n-num3)\n",
    "\n",
    "def generate_train_data_point(numbers):\n",
    "    data_point = [random.choice(train_digits[numbers[i]]) for i in range(5)]\n",
    "    return (torch.cat(data_point),numbers[5])\n",
    "\n",
    "def generate_test_data_point(numbers):\n",
    "    data_point = [random.choice(test_digits[numbers[i]]) for i in range(5)],\n",
    "    return (torch.cat(data_point),numbers[5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.6425, -0.7890, -1.0330],\n",
       "         [ 0.9182, -0.5349,  0.2251]],\n",
       "\n",
       "        [[-0.6425, -0.7890, -1.0330],\n",
       "         [ 0.9182, -0.5349,  0.2251]],\n",
       "\n",
       "        [[-0.6425, -0.7890, -1.0330],\n",
       "         [ 0.9182, -0.5349,  0.2251]],\n",
       "\n",
       "        [[-0.6425, -0.7890, -1.0330],\n",
       "         [ 0.9182, -0.5349,  0.2251]],\n",
       "\n",
       "        [[-0.6425, -0.7890, -1.0330],\n",
       "         [ 0.9182, -0.5349,  0.2251]]])"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn(2, 3)\n",
    "torch.cat([x[None,:,:] for i in range(5)], 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([28, 28])"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_digits[1][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data=[]\n",
    "for i in range(100000):\n",
    "    total = random.randint(4,16)\n",
    "    num = generate_num_set(total)\n",
    "    training_data += [generate_train_data_point(num)]\n",
    "    \n",
    "test_data=[]\n",
    "for i in range(1000):\n",
    "    total = random.randint(4,16)\n",
    "    num = generate_num_set(total)\n",
    "    test_data += [generate_test_data_point(num)]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_dataloader = DataLoader(training_data, batch_size=64, shuffle=True)\n",
    "test_dataloader = DataLoader(test_data, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader , test_dataloader = mns_dataloaders.three_channels()\n",
    "model = mns_models.three_channels_Net().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "# Get cpu, gpu or mps device for training.\n",
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### dealing with concatenated input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(torch.cat([train_digits[0][0],torch.zeros(10, 28),train_digits[0][1]]), cmap=\"gray\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def generate_num_set(n):\n",
    "    small = max(n-NUM_RANGE[1],0)\n",
    "    big = min(NUM_RANGE[1],n - small)\n",
    "    num1 = random.randint(small,big)\n",
    "    num2 = random.randint(small,big)\n",
    "    return (num1,n-num1,num2,n-num2)\n",
    "\n",
    "def generate_train_data_point(numbers):\n",
    "    im1 = torch.cat([random.choice(train_digits[numbers[0]]),torch.zeros(10, 28),random.choice(train_digits[numbers[1]])])\n",
    "    im2 = torch.cat([random.choice(train_digits[numbers[2]]),torch.zeros(10, 28),random.choice(train_digits[numbers[3]])])\n",
    "    data_point = [im1[None,:,:],im2[None,:,:]]\n",
    "    return (torch.cat(data_point),torch.tensor(numbers))\n",
    "\n",
    "def generate_test_data_point(numbers):\n",
    "    im1 = torch.cat([random.choice(test_digits[numbers[0]]),torch.zeros(8, 28),random.choice(test_digits[numbers[1]])])\n",
    "    im2 = torch.cat([random.choice(test_digits[numbers[2]]),torch.zeros(8, 28),random.choice(test_digits[numbers[3]])])\n",
    "    data_point = [im1[None,:,:],im2[None,:,:]]\n",
    "    return (torch.cat(data_point),torch.tensor(numbers))\n",
    "\n",
    "\n",
    "training_data=[]\n",
    "for i in range(100000):\n",
    "    total = random.randint(4,16)\n",
    "    num = generate_num_set(total)\n",
    "    training_data += [generate_train_data_point(num)]\n",
    "    \n",
    "test_data=[]\n",
    "for i in range(10000):\n",
    "    total = random.randint(4,16)\n",
    "    num = generate_num_set(total)\n",
    "    test_data += [generate_test_data_point(num)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "train_dataloader = DataLoader(training_data, batch_size=64, shuffle=True)\n",
    "test_dataloader = DataLoader(test_data, batch_size=64, shuffle=True)\n",
    "    \n",
    "class concat_Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(2, 6, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1 = nn.Linear(16 * 13 * 4, 480)\n",
    "        self.fc2 = nn.Linear(480, 250)\n",
    "        self.fc3 = nn.Linear(250, 40)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = torch.flatten(x, 1) # flatten all dimensions except batch\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "model = concat_Net().to(device)\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-3)\n",
    "def train(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    model.train()\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        X, y = X.to(device), y.to(device)# Compute prediction error\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred[:,-10:], y[:,-1])\n",
    "\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), (batch + 1) * len(X)\n",
    "            # print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "\n",
    "def test(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    model.eval()\n",
    "    test_loss, correct = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred[:,-10:], y[:,-1]).item()\n",
    "            correct += (pred[:,-10:].argmax(1) == y[:,-1]).type(torch.float).sum().item()\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    model.train()\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        X, y = X.to(device), y.to(device)# Compute prediction error\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred[:,:10], y[:,0])+loss_fn(pred[:,10:20], y[:,1])+loss_fn(pred[:,20:30], y[:,2])+loss_fn(pred[:,30:], y[:,3])\n",
    "\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), (batch + 1) * len(X)\n",
    "            # print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "\n",
    "def test(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    model.eval()\n",
    "    test_loss, correct = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred[:,-10:], y[:,-1]).item()\n",
    "            correct += (pred[:,-10:].argmax(1) == y[:,-1]).type(torch.float).sum().item()\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Given groups=1, weight of size [6, 2, 5, 5], expected input[64, 3, 28, 28] to have 2 channels, but got 3 channels instead",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[32], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(epochs):\n\u001b[1;32m      3\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mEpoch \u001b[39m\u001b[39m{\u001b[39;00mt\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m-------------------------------\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m----> 4\u001b[0m     train(train_dataloader, model, loss_fn, optimizer)\n\u001b[1;32m      5\u001b[0m     test(test_dataloader, model, loss_fn)\n\u001b[1;32m      6\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mDone!\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[31], line 10\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(dataloader, model, loss_fn, optimizer)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[39mfor\u001b[39;00m batch, (X, y) \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(dataloader):\n\u001b[1;32m      9\u001b[0m     X, y \u001b[39m=\u001b[39m X\u001b[39m.\u001b[39mto(device), y\u001b[39m.\u001b[39mto(device)\u001b[39m# Compute prediction error\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m     pred \u001b[39m=\u001b[39m model(X)\n\u001b[1;32m     11\u001b[0m     loss \u001b[39m=\u001b[39m loss_fn(pred[:,\u001b[39m-\u001b[39m\u001b[39m10\u001b[39m:], y[:,\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m])\n\u001b[1;32m     13\u001b[0m     \u001b[39m# Backpropagation\u001b[39;00m\n",
      "File \u001b[0;32m/usr/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/deeplearning/MISTI_Brazil_dPASP_research/MISTI_Brazil_dPASP_research/mns_models.py:16\u001b[0m, in \u001b[0;36mtwo_concat_Net.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[0;32m---> 16\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpool(F\u001b[39m.\u001b[39mrelu(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconv1(x)))\n\u001b[1;32m     17\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpool(F\u001b[39m.\u001b[39mrelu(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconv2(x)))\n\u001b[1;32m     18\u001b[0m     x \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mflatten(x, \u001b[39m1\u001b[39m) \u001b[39m# flatten all dimensions except batch\u001b[39;00m\n",
      "File \u001b[0;32m/usr/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/usr/lib/python3.11/site-packages/torch/nn/modules/conv.py:463\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    462\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 463\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_conv_forward(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "File \u001b[0;32m/usr/lib/python3.11/site-packages/torch/nn/modules/conv.py:459\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    455\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode \u001b[39m!=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mzeros\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m    456\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39mconv2d(F\u001b[39m.\u001b[39mpad(\u001b[39minput\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode),\n\u001b[1;32m    457\u001b[0m                     weight, bias, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstride,\n\u001b[1;32m    458\u001b[0m                     _pair(\u001b[39m0\u001b[39m), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdilation, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgroups)\n\u001b[0;32m--> 459\u001b[0m \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mconv2d(\u001b[39minput\u001b[39;49m, weight, bias, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstride,\n\u001b[1;32m    460\u001b[0m                 \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdilation, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgroups)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Given groups=1, weight of size [6, 2, 5, 5], expected input[64, 3, 28, 28] to have 2 channels, but got 3 channels instead"
     ]
    }
   ],
   "source": [
    "\n",
    "epochs = 50\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train(train_dataloader, model, loss_fn, optimizer)\n",
    "    test(test_dataloader, model, loss_fn)\n",
    "print(\"Done!\")\n",
    "\n",
    "# torch.save(model.state_dict(), \"model.pth\")\n",
    "# print(\"Saved PyTorch Model State to model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(6, device='cuda:0') tensor(5)\n",
      "tensor(6, device='cuda:0') tensor(0)\n",
      "tensor(6, device='cuda:0') tensor(5)\n",
      "tensor(8, device='cuda:0') tensor(5)\n",
      "tensor(2, device='cuda:0') tensor(7)\n",
      "tensor(9, device='cuda:0') tensor(4)\n",
      "tensor(4, device='cuda:0') tensor(9)\n",
      "tensor(6, device='cuda:0') tensor(0)\n",
      "tensor(9, device='cuda:0') tensor(8)\n",
      "tensor(2, device='cuda:0') tensor(0)\n",
      "tensor(2, device='cuda:0') tensor(0)\n",
      "tensor(8, device='cuda:0') tensor(4)\n",
      "tensor(8, device='cuda:0') tensor(9)\n",
      "tensor(8, device='cuda:0') tensor(1)\n",
      "tensor(8, device='cuda:0') tensor(7)\n",
      "tensor(8, device='cuda:0') tensor(9)\n",
      "tensor(3, device='cuda:0') tensor(8)\n",
      "tensor(8, device='cuda:0') tensor(9)\n",
      "tensor(3, device='cuda:0') tensor(7)\n",
      "tensor(2, device='cuda:0') tensor(3)\n",
      "tensor(8, device='cuda:0') tensor(9)\n",
      "tensor(6, device='cuda:0') tensor(1)\n",
      "tensor(6, device='cuda:0') tensor(0)\n",
      "tensor(9, device='cuda:0') tensor(4)\n",
      "tensor(6, device='cuda:0') tensor(0)\n",
      "tensor(9, device='cuda:0') tensor(0)\n",
      "tensor(8, device='cuda:0') tensor(0)\n",
      "tensor(0, device='cuda:0') tensor(5)\n",
      "tensor(6, device='cuda:0') tensor(0)\n",
      "tensor(5, device='cuda:0') tensor(3)\n",
      "tensor(9, device='cuda:0') tensor(4)\n",
      "tensor(8, device='cuda:0') tensor(4)\n",
      "tensor(8, device='cuda:0') tensor(0)\n",
      "tensor(4, device='cuda:0') tensor(0)\n",
      "tensor(8, device='cuda:0') tensor(4)\n",
      "tensor(8, device='cuda:0') tensor(9)\n",
      "tensor(3, device='cuda:0') tensor(8)\n",
      "tensor(8, device='cuda:0') tensor(9)\n",
      "tensor(8, device='cuda:0') tensor(1)\n",
      "tensor(9, device='cuda:0') tensor(3)\n",
      "tensor(8, device='cuda:0') tensor(9)\n",
      "tensor(8, device='cuda:0') tensor(7)\n",
      "tensor(5, device='cuda:0') tensor(8)\n",
      "tensor(8, device='cuda:0') tensor(9)\n"
     ]
    }
   ],
   "source": [
    "for i in range(100):\n",
    "    pred = model(test_data[i][0][None,:,:].to(device))\n",
    "    truth = test_data[i][1]\n",
    "    if pred[0][:10].argmax() != truth[0]:\n",
    "        print(pred[0][:10].argmax(),truth[0])\n",
    "    if pred[0][10:20].argmax() != truth[1]:\n",
    "        print(pred[0][10:20].argmax(),truth[1])\n",
    "    if pred[0][20:30].argmax() != truth[2]:\n",
    "        print(pred[0][20:30].argmax(),truth[2])\n",
    "    if pred[0][30:].argmax() != truth[3]:\n",
    "        print(pred[0][30:].argmax(),truth[3])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### model when input is 5 channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_train_data_point(numbers):\n",
    "    data_point = [random.choice(train_digits[numbers[i]])[None,:,:] for i in range(5)]\n",
    "    return (torch.cat(data_point),numbers[5])\n",
    "\n",
    "def generate_test_data_point(numbers):\n",
    "    data_point = [random.choice(test_digits[numbers[i]])[None,:,:] for i in range(5)]\n",
    "    return (torch.cat(data_point),numbers[5])\n",
    "\n",
    "training_data=[]\n",
    "for i in range(100000):\n",
    "    total = random.randint(4,16)\n",
    "    num = generate_num_set(total)\n",
    "    training_data += [generate_train_data_point(num)]\n",
    "    \n",
    "test_data=[]\n",
    "for i in range(10000):\n",
    "    total = random.randint(4,16)\n",
    "    num = generate_num_set(total)\n",
    "    test_data += [generate_test_data_point(num)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 28, 28])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "train_dataloader = DataLoader(training_data, batch_size=64, shuffle=True)\n",
    "test_dataloader = DataLoader(test_data, batch_size=64, shuffle=True)\n",
    "    \n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(5, 6, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1 = nn.Linear(16 * 4 * 4, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = torch.flatten(x, 1) # flatten all dimensions except batch\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "model = Net().to(device)\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-3)\n",
    "\n",
    "def train(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    model.train()\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "\n",
    "        # Compute prediction error\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), (batch + 1) * len(X)\n",
    "            # print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "\n",
    "def test(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    model.eval()\n",
    "    test_loss, correct = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### one channel normal network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(28*28*5, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 10)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-3)\n",
    "\n",
    "def train(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    model.train()\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "\n",
    "        # Compute prediction error\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), (batch + 1) * len(X)\n",
    "            # print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "\n",
    "def test(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    model.eval()\n",
    "    test_loss, correct = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### see results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 12.2%, Avg loss: 2.294277 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 11.3%, Avg loss: 2.290142 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 14.3%, Avg loss: 2.286663 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 14.3%, Avg loss: 2.286643 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 14.3%, Avg loss: 2.285849 \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 14.3%, Avg loss: 2.285073 \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 14.3%, Avg loss: 2.286570 \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 14.3%, Avg loss: 2.283767 \n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 14.3%, Avg loss: 2.285037 \n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 14.3%, Avg loss: 2.284414 \n",
      "\n",
      "Epoch 11\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 14.3%, Avg loss: 2.284953 \n",
      "\n",
      "Epoch 12\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 14.3%, Avg loss: 2.285940 \n",
      "\n",
      "Epoch 13\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 14.3%, Avg loss: 2.282672 \n",
      "\n",
      "Epoch 14\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 14.3%, Avg loss: 2.282568 \n",
      "\n",
      "Epoch 15\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 14.3%, Avg loss: 2.280957 \n",
      "\n",
      "Epoch 16\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 14.3%, Avg loss: 2.281573 \n",
      "\n",
      "Epoch 17\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 14.3%, Avg loss: 2.278567 \n",
      "\n",
      "Epoch 18\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 14.3%, Avg loss: 2.278279 \n",
      "\n",
      "Epoch 19\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 14.3%, Avg loss: 2.275492 \n",
      "\n",
      "Epoch 20\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 14.3%, Avg loss: 2.274045 \n",
      "\n",
      "Epoch 21\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 14.2%, Avg loss: 2.267651 \n",
      "\n",
      "Epoch 22\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 14.4%, Avg loss: 2.260635 \n",
      "\n",
      "Epoch 23\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 15.5%, Avg loss: 2.249851 \n",
      "\n",
      "Epoch 24\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 15.5%, Avg loss: 2.236167 \n",
      "\n",
      "Epoch 25\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 14.9%, Avg loss: 2.221676 \n",
      "\n",
      "Epoch 26\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 15.8%, Avg loss: 2.207934 \n",
      "\n",
      "Epoch 27\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 15.7%, Avg loss: 2.194890 \n",
      "\n",
      "Epoch 28\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 15.5%, Avg loss: 2.181666 \n",
      "\n",
      "Epoch 29\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 15.9%, Avg loss: 2.168909 \n",
      "\n",
      "Epoch 30\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 16.2%, Avg loss: 2.156941 \n",
      "\n",
      "Epoch 31\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 17.0%, Avg loss: 2.145110 \n",
      "\n",
      "Epoch 32\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 16.2%, Avg loss: 2.138516 \n",
      "\n",
      "Epoch 33\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 16.6%, Avg loss: 2.130553 \n",
      "\n",
      "Epoch 34\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 15.7%, Avg loss: 2.124586 \n",
      "\n",
      "Epoch 35\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 17.2%, Avg loss: 2.117692 \n",
      "\n",
      "Epoch 36\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 17.6%, Avg loss: 2.108433 \n",
      "\n",
      "Epoch 37\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 16.2%, Avg loss: 2.115840 \n",
      "\n",
      "Epoch 38\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 19.0%, Avg loss: 2.099398 \n",
      "\n",
      "Epoch 39\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 18.9%, Avg loss: 2.099781 \n",
      "\n",
      "Epoch 40\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 17.6%, Avg loss: 2.089993 \n",
      "\n",
      "Epoch 41\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 17.5%, Avg loss: 2.092814 \n",
      "\n",
      "Epoch 42\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 18.4%, Avg loss: 2.089482 \n",
      "\n",
      "Epoch 43\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 19.5%, Avg loss: 2.078232 \n",
      "\n",
      "Epoch 44\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 18.0%, Avg loss: 2.085047 \n",
      "\n",
      "Epoch 45\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 19.2%, Avg loss: 2.075903 \n",
      "\n",
      "Epoch 46\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 19.2%, Avg loss: 2.071089 \n",
      "\n",
      "Epoch 47\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 19.3%, Avg loss: 2.063409 \n",
      "\n",
      "Epoch 48\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 17.7%, Avg loss: 2.073223 \n",
      "\n",
      "Epoch 49\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 20.1%, Avg loss: 2.068096 \n",
      "\n",
      "Epoch 50\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 19.9%, Avg loss: 2.051354 \n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "epochs = 50\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train(train_dataloader, model, loss_fn, optimizer)\n",
    "    test(test_dataloader, model, loss_fn)\n",
    "print(\"Done!\")\n",
    "\n",
    "# torch.save(model.state_dict(), \"model.pth\")\n",
    "# print(\"Saved PyTorch Model State to model.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train data for three digits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def generate_num_set(n):\n",
    "    small = max(n-NUM_RANGE[1],0)\n",
    "    big = min(NUM_RANGE[1],n - small)\n",
    "    num1 = random.randint(small,big)\n",
    "    num2 = random.randint(small,big)\n",
    "    return (num1,n-num1,num2,n-num2)\n",
    "\n",
    "def generate_train_data_point(numbers):\n",
    "    data_point = [random.choice(train_digits[numbers[i]])[None,:,:] for i in range(3)]\n",
    "    return (torch.cat(data_point),numbers[3])\n",
    "\n",
    "def generate_test_data_point(numbers):\n",
    "    data_point =[random.choice(test_digits[numbers[i]])[None,:,:] for i in range(3)]\n",
    "    return (torch.cat(data_point),numbers[3])\n",
    "\n",
    "training_data=[]\n",
    "for i in range(100000):\n",
    "    total = random.randint(4,16)\n",
    "    num = generate_num_set(total)\n",
    "    training_data += [generate_train_data_point(num)]\n",
    "    \n",
    "test_data=[]\n",
    "for i in range(10000):\n",
    "    total = random.randint(4,16)\n",
    "    num = generate_num_set(total)\n",
    "    test_data += [generate_test_data_point(num)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "train_dataloader = DataLoader(training_data, batch_size=64, shuffle=True)\n",
    "test_dataloader = DataLoader(test_data, batch_size=64, shuffle=True)\n",
    "    \n",
    "class three_Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1 = nn.Linear(16 * 4 * 4, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = torch.flatten(x, 1) # flatten all dimensions except batch\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "model = three_Net().to(device)\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-3)\n",
    "\n",
    "def train(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    model.train()\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "\n",
    "        # Compute prediction error\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), (batch + 1) * len(X)\n",
    "            # print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "\n",
    "def test(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    model.eval()\n",
    "    test_loss, correct = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
