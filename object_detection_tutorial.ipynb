{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Object detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "class PennFudanDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, root, transforms):\n",
    "        self.root = root\n",
    "        self.transforms = transforms\n",
    "        # load all image files, sorting them to\n",
    "        # ensure that they are aligned\n",
    "        self.imgs = list(sorted(os.listdir(os.path.join(root, \"PNGImages\"))))\n",
    "        self.masks = list(sorted(os.listdir(os.path.join(root, \"PedMasks\"))))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # load images and masks\n",
    "        img_path = os.path.join(self.root, \"PNGImages\", self.imgs[idx])\n",
    "        mask_path = os.path.join(self.root, \"PedMasks\", self.masks[idx])\n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "        # note that we haven't converted the mask to RGB,\n",
    "        # because each color corresponds to a different instance\n",
    "        # with 0 being background\n",
    "        mask = Image.open(mask_path)\n",
    "        # convert the PIL Image into a numpy array\n",
    "        mask = np.array(mask)\n",
    "        # instances are encoded as different colors\n",
    "        obj_ids = np.unique(mask)\n",
    "        # first id is the background, so remove it\n",
    "        obj_ids = obj_ids[1:]\n",
    "\n",
    "        # split the color-encoded mask into a set\n",
    "        # of binary masks\n",
    "        masks = mask == obj_ids[:, None, None]\n",
    "\n",
    "        # get bounding box coordinates for each mask\n",
    "        num_objs = len(obj_ids)\n",
    "        boxes = []\n",
    "        for i in range(num_objs):\n",
    "            pos = np.nonzero(masks[i])\n",
    "            xmin = np.min(pos[1])\n",
    "            xmax = np.max(pos[1])\n",
    "            ymin = np.min(pos[0])\n",
    "            ymax = np.max(pos[0])\n",
    "            boxes.append([xmin, ymin, xmax, ymax])\n",
    "\n",
    "        # convert everything into a torch.Tensor\n",
    "        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
    "        # there is only one class\n",
    "        labels = torch.ones((num_objs,), dtype=torch.int64)\n",
    "        masks = torch.as_tensor(masks, dtype=torch.uint8)\n",
    "\n",
    "        image_id = torch.tensor([idx])\n",
    "        area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n",
    "        # suppose all instances are not crowd\n",
    "        iscrowd = torch.zeros((num_objs,), dtype=torch.int64)\n",
    "\n",
    "        target = {}\n",
    "        target[\"boxes\"] = boxes\n",
    "        target[\"labels\"] = labels\n",
    "        target[\"masks\"] = masks\n",
    "        target[\"image_id\"] = image_id\n",
    "        target[\"area\"] = area\n",
    "        target[\"iscrowd\"] = iscrowd\n",
    "\n",
    "        if self.transforms is not None:\n",
    "            img, target = self.transforms(img, target)\n",
    "\n",
    "        return img, target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.imgs)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finetuning a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "\n",
    "# load a model pre-trained on COCO\n",
    "model = torchvision.models.detection.fasterrcnn_resnet50_fpn(weights=\"DEFAULT\")\n",
    "\n",
    "# replace the classifier with a new one, that has\n",
    "# num_classes which is user-defined\n",
    "num_classes = 2  # 1 class (person) + background\n",
    "# get number of input features for the classifier\n",
    "in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "# replace the pre-trained head with a new one\n",
    "model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding a different backbone to a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "from torchvision.models.detection import FasterRCNN\n",
    "from torchvision.models.detection.rpn import AnchorGenerator\n",
    "\n",
    "# load a pre-trained model for classification and return\n",
    "# only the features\n",
    "backbone = torchvision.models.mobilenet_v2(weights=\"DEFAULT\").features\n",
    "# FasterRCNN needs to know the number of\n",
    "# output channels in a backbone. For mobilenet_v2, it's 1280\n",
    "# so we need to add it here\n",
    "backbone.out_channels = 1280\n",
    "\n",
    "# let's make the RPN generate 5 x 3 anchors per spatial\n",
    "# location, with 5 different sizes and 3 different aspect\n",
    "# ratios. We have a Tuple[Tuple[int]] because each feature\n",
    "# map could potentially have different sizes and\n",
    "# aspect ratios\n",
    "anchor_generator = AnchorGenerator(sizes=((32, 64, 128, 256, 512),),\n",
    "                                   aspect_ratios=((0.5, 1.0, 2.0),))\n",
    "\n",
    "# let's define what are the feature maps that we will\n",
    "# use to perform the region of interest cropping, as well as\n",
    "# the size of the crop after rescaling.\n",
    "# if your backbone returns a Tensor, featmap_names is expected to\n",
    "# be [0]. More generally, the backbone should return an\n",
    "# OrderedDict[Tensor], and in featmap_names you can choose which\n",
    "# feature maps to use.\n",
    "roi_pooler = torchvision.ops.MultiScaleRoIAlign(featmap_names=['0'],\n",
    "                                                output_size=7,\n",
    "                                                sampling_ratio=2)\n",
    "\n",
    "# put the pieces together inside a FasterRCNN model\n",
    "model = FasterRCNN(backbone,\n",
    "                   num_classes=2,\n",
    "                   rpn_anchor_generator=anchor_generator,\n",
    "                   box_roi_pool=roi_pooler)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instance segmentation model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from torchvision.models.detection.mask_rcnn import MaskRCNNPredictor\n",
    "\n",
    "\n",
    "def get_model_instance_segmentation(num_classes):\n",
    "    # load an instance segmentation model pre-trained on COCO\n",
    "    model = torchvision.models.detection.maskrcnn_resnet50_fpn(weights=\"DEFAULT\")\n",
    "\n",
    "    # get number of input features for the classifier\n",
    "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "    # replace the pre-trained head with a new one\n",
    "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "\n",
    "    # now get the number of input features for the mask classifier\n",
    "    in_features_mask = model.roi_heads.mask_predictor.conv5_mask.in_channels\n",
    "    hidden_layer = 256\n",
    "    # and replace the mask predictor with a new one\n",
    "    model.roi_heads.mask_predictor = MaskRCNNPredictor(in_features_mask,\n",
    "                                                       hidden_layer,\n",
    "                                                       num_classes)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transforms as T\n",
    "\n",
    "def get_transform(train):\n",
    "    transforms = []\n",
    "    transforms.append(T.PILToTensor())\n",
    "    transforms.append(T.ConvertImageDtype(torch.float))\n",
    "    if train:\n",
    "        transforms.append(T.RandomHorizontalFlip(0.5))\n",
    "    return T.Compose(transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from engine import train_one_epoch, evaluate\n",
    "import utils\n",
    "\n",
    "\n",
    "def main():\n",
    "    # train on the GPU or on the CPU, if a GPU is not available\n",
    "    device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "    # our dataset has two classes only - background and person\n",
    "    num_classes = 2\n",
    "    # use our dataset and defined transformations\n",
    "    dataset = PennFudanDataset('PennFudanPed', get_transform(train=True))\n",
    "    dataset_test = PennFudanDataset('PennFudanPed', get_transform(train=False))\n",
    "\n",
    "    # split the dataset in train and test set\n",
    "    indices = torch.randperm(len(dataset)).tolist()\n",
    "    dataset = torch.utils.data.Subset(dataset, indices[:-50])\n",
    "    dataset_test = torch.utils.data.Subset(dataset_test, indices[-50:])\n",
    "\n",
    "    # define training and validation data loaders\n",
    "    data_loader = torch.utils.data.DataLoader(\n",
    "        dataset, batch_size=2, shuffle=True, num_workers=4,\n",
    "        collate_fn=utils.collate_fn)\n",
    "\n",
    "    data_loader_test = torch.utils.data.DataLoader(\n",
    "        dataset_test, batch_size=1, shuffle=False, num_workers=4,\n",
    "        collate_fn=utils.collate_fn)\n",
    "\n",
    "    # get the model using our helper function\n",
    "    model = get_model_instance_segmentation(num_classes)\n",
    "\n",
    "    # move model to the right device\n",
    "    model.to(device)\n",
    "\n",
    "    # construct an optimizer\n",
    "    params = [p for p in model.parameters() if p.requires_grad]\n",
    "    optimizer = torch.optim.SGD(params, lr=0.005,\n",
    "                                momentum=0.9, weight_decay=0.0005)\n",
    "    # and a learning rate scheduler\n",
    "    lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer,\n",
    "                                                   step_size=3,\n",
    "                                                   gamma=0.1)\n",
    "\n",
    "    # let's train it for 10 epochs\n",
    "    num_epochs = 10\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        # train for one epoch, printing every 10 iterations\n",
    "        train_one_epoch(model, optimizer, data_loader, device, epoch, print_freq=10)\n",
    "        # update the learning rate\n",
    "        lr_scheduler.step()\n",
    "        # evaluate on the test dataset\n",
    "        evaluate(model, data_loader_test, device=device)\n",
    "\n",
    "    print(\"That's it!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from engine import train_one_epoch, evaluate\n",
    "import utils\n",
    "\n",
    "# train on the GPU or on the CPU, if a GPU is not available\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "# our dataset has two classes only - background and person\n",
    "num_classes = 2\n",
    "# use our dataset and defined transformations\n",
    "dataset = PennFudanDataset('PennFudanPed', get_transform(train=True))\n",
    "dataset_test = PennFudanDataset('PennFudanPed', get_transform(train=False))\n",
    "\n",
    "# split the dataset in train and test set\n",
    "indices = torch.randperm(len(dataset)).tolist()\n",
    "dataset = torch.utils.data.Subset(dataset, indices[:-50])\n",
    "dataset_test = torch.utils.data.Subset(dataset_test, indices[-50:])\n",
    "\n",
    "# define training and validation data loaders\n",
    "data_loader = torch.utils.data.DataLoader(\n",
    "    dataset, batch_size=2, shuffle=True, num_workers=4,\n",
    "    collate_fn=utils.collate_fn)\n",
    "\n",
    "data_loader_test = torch.utils.data.DataLoader(\n",
    "    dataset_test, batch_size=1, shuffle=False, num_workers=4,\n",
    "    collate_fn=utils.collate_fn)\n",
    "\n",
    "# get the model using our helper function\n",
    "model = get_model_instance_segmentation(num_classes)\n",
    "\n",
    "# move model to the right device\n",
    "model.to(device)\n",
    "\n",
    "# construct an optimizer\n",
    "params = [p for p in model.parameters() if p.requires_grad]\n",
    "optimizer = torch.optim.SGD(params, lr=0.005,\n",
    "                            momentum=0.9, weight_decay=0.0005)\n",
    "# and a learning rate scheduler\n",
    "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer,\n",
    "                                                step_size=3,\n",
    "                                                gamma=0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [0]  [ 0/60]  eta: 0:25:43  lr: 0.000090  loss: 3.5659 (3.5659)  loss_classifier: 0.6248 (0.6248)  loss_box_reg: 0.4850 (0.4850)  loss_mask: 2.4076 (2.4076)  loss_objectness: 0.0433 (0.0433)  loss_rpn_box_reg: 0.0052 (0.0052)  time: 25.7258  data: 0.1619\n",
      "Epoch: [0]  [10/60]  eta: 0:19:20  lr: 0.000936  loss: 1.5622 (2.1316)  loss_classifier: 0.4258 (0.4156)  loss_box_reg: 0.2766 (0.3401)  loss_mask: 0.8417 (1.3353)  loss_objectness: 0.0292 (0.0350)  loss_rpn_box_reg: 0.0052 (0.0056)  time: 23.2172  data: 0.0170\n",
      "Epoch: [0]  [20/60]  eta: 0:15:07  lr: 0.001783  loss: 0.9015 (1.4150)  loss_classifier: 0.2194 (0.2894)  loss_box_reg: 0.2534 (0.2833)  loss_mask: 0.3603 (0.8137)  loss_objectness: 0.0130 (0.0229)  loss_rpn_box_reg: 0.0054 (0.0057)  time: 22.5444  data: 0.0027\n",
      "Epoch: [0]  [30/60]  eta: 0:11:28  lr: 0.002629  loss: 0.5790 (1.1499)  loss_classifier: 0.1104 (0.2287)  loss_box_reg: 0.2233 (0.2802)  loss_mask: 0.2178 (0.6175)  loss_objectness: 0.0048 (0.0172)  loss_rpn_box_reg: 0.0054 (0.0062)  time: 22.8259  data: 0.0028\n",
      "Epoch: [0]  [40/60]  eta: 0:07:34  lr: 0.003476  loss: 0.4476 (0.9655)  loss_classifier: 0.0583 (0.1847)  loss_box_reg: 0.1959 (0.2545)  loss_mask: 0.1570 (0.5062)  loss_objectness: 0.0043 (0.0140)  loss_rpn_box_reg: 0.0051 (0.0061)  time: 22.7300  data: 0.0028\n",
      "Epoch: [0]  [50/60]  eta: 0:03:47  lr: 0.004323  loss: 0.3758 (0.8486)  loss_classifier: 0.0435 (0.1576)  loss_box_reg: 0.1478 (0.2329)  loss_mask: 0.1525 (0.4392)  loss_objectness: 0.0022 (0.0119)  loss_rpn_box_reg: 0.0062 (0.0071)  time: 22.3190  data: 0.0028\n",
      "Epoch: [0]  [59/60]  eta: 0:00:23  lr: 0.005000  loss: 0.3664 (0.7815)  loss_classifier: 0.0435 (0.1424)  loss_box_reg: 0.1372 (0.2200)  loss_mask: 0.1746 (0.4013)  loss_objectness: 0.0021 (0.0106)  loss_rpn_box_reg: 0.0072 (0.0073)  time: 23.3821  data: 0.0028\n",
      "Epoch: [0] Total time: 0:23:00 (23.0144 s / it)\n"
     ]
    }
   ],
   "source": [
    "# train for one epoch, printing every 10 iterations\n",
    "train_one_epoch(model, optimizer, data_loader, device, 0, print_freq=10)\n",
    "# update the learning rate\n",
    "lr_scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating index...\n",
      "index created!\n",
      "[{'image_id': tensor([84]), 'category_id': 1, 'bbox': [87.1139144897461, 58.53897476196289, 142.0745849609375, 292.2987060546875], 'score': 0.9742231369018555}, {'image_id': tensor([84]), 'category_id': 1, 'bbox': [244.5027618408203, 48.318389892578125, 70.18928527832031, 325.9126281738281], 'score': 0.9169223308563232}, {'image_id': tensor([84]), 'category_id': 1, 'bbox': [239.25082397460938, 11.112505912780762, 132.98916625976562, 364.8874816894531], 'score': 0.6184369921684265}, {'image_id': tensor([84]), 'category_id': 1, 'bbox': [116.02566528320312, 50.16263961791992, 81.24557495117188, 255.0624237060547], 'score': 0.20641756057739258}]\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "Results do not correspond to current coco set",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# evaluate on the test dataset\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m evaluate(model, data_loader_test, device\u001b[39m=\u001b[39;49mdevice)\n",
      "File \u001b[0;32m/usr/lib/python3.11/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[39mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/deeplearning/MISTI_Brazil_dPASP_research/engine.py:102\u001b[0m, in \u001b[0;36mevaluate\u001b[0;34m(model, data_loader, device)\u001b[0m\n\u001b[1;32m    100\u001b[0m res \u001b[39m=\u001b[39m {target[\u001b[39m\"\u001b[39m\u001b[39mimage_id\u001b[39m\u001b[39m\"\u001b[39m]: output \u001b[39mfor\u001b[39;00m target, output \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(targets, outputs)}\n\u001b[1;32m    101\u001b[0m evaluator_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[0;32m--> 102\u001b[0m coco_evaluator\u001b[39m.\u001b[39;49mupdate(res)\n\u001b[1;32m    103\u001b[0m evaluator_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime() \u001b[39m-\u001b[39m evaluator_time\n\u001b[1;32m    104\u001b[0m metric_logger\u001b[39m.\u001b[39mupdate(model_time\u001b[39m=\u001b[39mmodel_time, evaluator_time\u001b[39m=\u001b[39mevaluator_time)\n",
      "File \u001b[0;32m~/deeplearning/MISTI_Brazil_dPASP_research/coco_eval.py:35\u001b[0m, in \u001b[0;36mCocoEvaluator.update\u001b[0;34m(self, predictions)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[39mprint\u001b[39m(results)\n\u001b[1;32m     34\u001b[0m \u001b[39mwith\u001b[39;00m redirect_stdout(io\u001b[39m.\u001b[39mStringIO()):\n\u001b[0;32m---> 35\u001b[0m     coco_dt \u001b[39m=\u001b[39m COCO\u001b[39m.\u001b[39;49mloadRes(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcoco_gt, results) \u001b[39mif\u001b[39;00m results \u001b[39melse\u001b[39;00m COCO()\n\u001b[1;32m     36\u001b[0m     \u001b[39mprint\u001b[39m(coco_dt)\n\u001b[1;32m     37\u001b[0m coco_eval \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcoco_eval[iou_type]\n",
      "File \u001b[0;32m/usr/lib/python3.11/site-packages/pycocotools/coco.py:327\u001b[0m, in \u001b[0;36mCOCO.loadRes\u001b[0;34m(self, resFile)\u001b[0m\n\u001b[1;32m    325\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mtype\u001b[39m(anns) \u001b[39m==\u001b[39m \u001b[39mlist\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mresults in not an array of objects\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m    326\u001b[0m annsImgIds \u001b[39m=\u001b[39m [ann[\u001b[39m'\u001b[39m\u001b[39mimage_id\u001b[39m\u001b[39m'\u001b[39m] \u001b[39mfor\u001b[39;00m ann \u001b[39min\u001b[39;00m anns]\n\u001b[0;32m--> 327\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mset\u001b[39m(annsImgIds) \u001b[39m==\u001b[39m (\u001b[39mset\u001b[39m(annsImgIds) \u001b[39m&\u001b[39m \u001b[39mset\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgetImgIds())), \\\n\u001b[1;32m    328\u001b[0m        \u001b[39m'\u001b[39m\u001b[39mResults do not correspond to current coco set\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m    329\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mcaption\u001b[39m\u001b[39m'\u001b[39m \u001b[39min\u001b[39;00m anns[\u001b[39m0\u001b[39m]:\n\u001b[1;32m    330\u001b[0m     imgIds \u001b[39m=\u001b[39m \u001b[39mset\u001b[39m([img[\u001b[39m'\u001b[39m\u001b[39mid\u001b[39m\u001b[39m'\u001b[39m] \u001b[39mfor\u001b[39;00m img \u001b[39min\u001b[39;00m res\u001b[39m.\u001b[39mdataset[\u001b[39m'\u001b[39m\u001b[39mimages\u001b[39m\u001b[39m'\u001b[39m]]) \u001b[39m&\u001b[39m \u001b[39mset\u001b[39m([ann[\u001b[39m'\u001b[39m\u001b[39mimage_id\u001b[39m\u001b[39m'\u001b[39m] \u001b[39mfor\u001b[39;00m ann \u001b[39min\u001b[39;00m anns])\n",
      "\u001b[0;31mAssertionError\u001b[0m: Results do not correspond to current coco set"
     ]
    }
   ],
   "source": [
    "# evaluate on the test dataset\n",
    "evaluate(model, data_loader_test, device=device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
