#python
import torch
import torchvision
import random

NUM_RANGE = (0,9)

# Digit classification network definition.
class Net(torch.nn.Module):
  def __init__(self):
    super().__init__()
    self.encoder = torch.nn.Sequential(
      torch.nn.Conv2d(1, 6, 5),
      torch.nn.MaxPool2d(2, 2),
      torch.nn.ReLU(True),
      torch.nn.Conv2d(6, 16, 5),
      torch.nn.MaxPool2d(2, 2),
      torch.nn.ReLU(True)
    )
    self.classifier = torch.nn.Sequential(
      torch.nn.Linear(16 * 4 * 4, 120),
      torch.nn.ReLU(),
      torch.nn.Linear(120, 84),
      torch.nn.ReLU(),
      torch.nn.Linear(84, 10),
      torch.nn.Softmax(1)
    )

  def forward(self, x):
    x = self.encoder(x)
    x = x.view(-1, 16 * 4 * 4)
    x = self.classifier(x)
    return x

# Return an instance of Net.
def digit_net(): return Net()

# Function to transform the dataset
transform = torchvision.transforms.Compose(
    [torchvision.transforms.ToTensor(),
     torchvision.transforms.Normalize((0.5), (0.5))])

# Download training data from open datasets.
training_data_atoms = torchvision.datasets.MNIST(
    root="data",
    train=True,
    download=True,
    transform=transform,
)

# Download test data from open datasets.
test_data_atoms = torchvision.datasets.MNIST(
    root="data",
    train=False,
    download=True,
    transform=transform,
)

test_digits= [[] for i in range(10)] # the digits can be one of 0,1,..9. Each array will store images of each separate digit.
for img, label in test_data_atoms:
    test_digits[label]+=img

train_digits= [[] for i in range(10)]
for img, label in training_data_atoms:
    train_digits[label]+=img

def generate_num_set(n): # given what the sum of each pair should be, this outputs a set of 6 numbers where each pair adds up to the sum.
    small = max(n-NUM_RANGE[1],0)
    big = min(NUM_RANGE[1],n - small)
    num1 = random.randint(small,big)
    num2 = random.randint(small,big)
    num3 = random.randint(small,big)
    return (num1,n-num1,num2,n-num2,num3,n-num3)

def generate_train_data_point(numbers):
    data_point = [random.choice(train_digits[numbers[i]])[None,:,:] for i in range(5)]
    return data_point+[numbers[5]] # generates training data where each training point consists of 5 images of digits, and the last is an integer which the algorithm should guess.

def generate_test_data_point(numbers):
    data_point = [random.choice(test_digits[numbers[i]])[None,:,:] for i in range(5)]
    return data_point+[numbers[5]] # generates test data in a similar way to training data.

training_data=[]
for i in range(1000):
    total = random.randint(3,15) # The total sum of the two digits is set to be between 3 and 15. (If we allow 0 or 1, it will be very boring because all the numbers will be 0 or 1 in this case.)
    num = generate_num_set(total)
    training_data += [generate_train_data_point(num)]
    
test_data=[]
for i in range(10):
    total = random.randint(3,15)
    num = generate_num_set(total)
    test_data += [generate_test_data_point(num)]


# MNIST images for the train set.
def mnist_images_train(num):
  return torch.cat([training_data[i][int(num)] for i in range(len(training_data))])[:,None,:,:]
# MNIST images for the test set.
def mnist_images_test(num): 
  with open("debug.csv",'w',newline = '') as csvfile:
    csvwriter = csv.writer(csvfile)
    csvwriter.writerow(["test",torch.cat([test_data[i][int(num)] for i in range(len(test_data))])[:,None,:,:].shape]) 
  return torch.cat([test_data[i][int(num)] for i in range(len(test_data))])[:,None,:,:]

# Observed atoms for training.
def mnist_labels_train():
  # we observe the labels for the data, which is the last number the system should predict. 
  # we also observe that the sum of the first pair is the same as the sum of the second pair. This is assumed in the data that we feed the model, but te model needs to learn that this is always true.
  labels = [training_data[i][-1] for i in range(len(training_data))]
  return [[f"last_num({x})","equal"] for x in labels]

# keep labels for test data
import csv
def note_label_for_test(test_data): # we write the ground truth of predicted values for the test set, so that it can be opened when running implement_dpasp.py to test accuracy.
  with open("label.csv",'w',newline = '') as csvfile:
    csvwriter = csv.writer(csvfile)
    for i in range(len(test_data)):
      csvwriter.writerow([test_data[i][-1]]) 

# run the function to write the ground truth in label.csv
note_label_for_test(test_data)

#end.

% Data of the first digit.
input(0) ~ test(@mnist_images_test(0)), train(@mnist_images_train(0)).
% Data of the second digit.
input(1) ~ test(@mnist_images_test(1)), train(@mnist_images_train(1)).
% Data of the third digit.
input(2) ~ test(@mnist_images_test(2)), train(@mnist_images_train(2)).
% Data of the fourth digit.
input(3) ~ test(@mnist_images_test(3)), train(@mnist_images_train(3)).
% Data of the fifth digit.
input(4) ~ test(@mnist_images_test(4)), train(@mnist_images_train(4)).

% Neural annotated disjunction over each digit from 0 to 9; use Adam as optimizer
% and a learning rate of 0.001.
?::digit(X, {0..9}) as @digit_net with optim = "Adam", lr = 0.001 :- input(X).

% Probability of the sum.
sum1(X) :- digit(0, A), digit(1, B), X = A+B.
sum2(Y) :- digit(2, A), digit(3, B), Y = A+B.
equal:- sum1(X),sum2(Y),X=Y.
last_num(L):- digit(4,A),sum1(X),L=X-A.

% Learn the parameters of the program from the "sum(X)" atoms.
#learn @mnist_labels_train, lr = 1., niters = 5, alg = "lagrange", batch = 10.
#semantics maxent.
% Ask for the probability of all groundings of last_num(L) given equal.
#query last_num(L)|equal.
